# -*- coding: utf-8 -*-
"""Hand Gesture Recognition Model Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1elnRLuo-GjGpD1WL-OnozzGHNoxE7CCl

The goal of this project is to train a Machine Learning algorithm capable of classifying images of different hand gestures,
such as a fist, palm, showing the thumb, and others.
The method used was Deep Learning with the help of Convolutional Neural Networks based on Tensorflow and Keras.
In the end, the algorithm successfully classifies different hand gestures images with enough confidence (>95%) based on a Deep Learning model.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
#from google.colab import files
import os

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt
import cv2
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

print(tf.__version__)

#!unzip 00.zip

imagepaths = []

for root, dirs, files in os.walk(".", topdown=False):
    for name in files:
        path = os.path.join(root, name)
        if path.endswith("png") and path.startswith(".\\leapGestRecog\\0"):
            imagepaths.append(path)

print(len(imagepaths))

print(imagepaths[0])


def plot_image(path):
    img = cv2.imread(path)
    img_cvt = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    print(img_cvt.shape)
    plt.grid(False)
    plt.imshow(img_cvt)
    plt.xlabel("Width")
    plt.ylabel("Height")
    plt.title("Image " + path)


plot_image(imagepaths[0])

X = []  # Image data
y = []  # Labels

# Loops through imagepaths to load images and labels into arrays
for path in imagepaths:
    img = cv2.imread(path)
    if (img is not None):
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Converts into the corret colorspace (GRAY)
        img = cv2.resize(img, (320, 120))  # Reduce image size so training can be faster
        X.append(img)

    # Processing label in image path
    if len(path.split("\\")) > 1:
        print(path.split("\\"))
        category = path.split("\\")[3]
        print(category.split("_")[0][1])
        label = int(category.split("_")[0][1])  # We need to convert 10_down to 00_down, or else it crashes
        y.append(label)

# Turn X and y into np.array to speed up train_test_split
X = np.array(X, dtype="uint8")
X = X.reshape(len(imagepaths), 120, 320, 1)  # Needed to reshape so CNN knows it's different images ----- CHECK..!!
y = np.array(y)

print("Images loaded: ", len(X))
print("Labels loaded: ", len(y))

print(y[0], imagepaths[0])

ts = 0.3  # Percentage of images that we want to use for testing. The rest is used for training.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42)

from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Dense, Flatten

model = Sequential()
model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(120, 320, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.compile(optimizer='adam',
              # Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.
              loss='sparse_categorical_crossentropy',  # Loss function, which tells us how bad our predictions are.
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, validation_data=(X_test, y_test))

model.save('handrecognition_model.h5')

test_loss, test_acc = model.evaluate(X_test, y_test)

print('Test accuracy: {:2.2f}%'.format(test_acc * 100))

predictions = model.predict(X_test)

np.argmax(predictions[0]), y_test[0]


def validate_9_images(predictions_array, true_label_array, img_array):
    # Array for pretty printing and then figure size
    class_names = ["down", "palm", "l", "fist", "fist_moved", "thumb", "index", "ok", "palm_moved", "c"]
    plt.figure(figsize=(15, 5))

    for i in range(1, 10):
        # Just assigning variables
        prediction = predictions_array[i]
        true_label = true_label_array[i]
        img = img_array[i]
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)

        # Plot in a good way
        plt.subplot(3, 3, i)
        plt.grid(False)
        plt.xticks([])
        plt.yticks([])
        plt.imshow(img, cmap=plt.cm.binary)

        predicted_label = np.argmax(prediction)  # Get index of the predicted label from prediction

        # Change color of title based on good prediction or not
        if predicted_label == true_label:
            color = 'blue'
        else:
            color = 'red'

        plt.xlabel("Predicted: {} {:2.0f}% (True: {})".format(class_names[predicted_label],
                                                              100 * np.max(prediction),
                                                              class_names[true_label]),
                   color=color)
    plt.show()


validate_9_images(predictions, y_test, X_test)

y_pred = np.argmax(predictions, axis=1)

pd.DataFrame(confusion_matrix(y_test, y_pred),
             columns=["Predicted Thumb Down", "Predicted Palm (H)", "Predicted L", "Predicted Fist (H)",
                      "Predicted Fist (V)", "Predicted Thumbs up", "Predicted Index", "Predicted OK",
                      "Predicted Palm (V)", "Predicted C"],
             index=["Actual Thumb Down", "Actual Palm (H)", "Actual L", "Actual Fist (H)", "Actual Fist (V)",
                    "Actual Thumbs up", "Actual Index", "Actual OK", "Actual Palm (V)", "Actual C"])